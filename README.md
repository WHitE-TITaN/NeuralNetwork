# NeuralNetwork
# Natural Language Processor

## Overview

This project implements a Natural Language Processor (NLP) using a combination of self-attention mechanisms, average pooling, and pre-trained GloVe embeddings. The architecture is designed to be simple yet effective, with neural networks and backpropagation used for training to solve various NLP tasks.

## Features

- **Self-Attention Mechanism:** Captures long-range dependencies in text, allowing the model to focus on important words across sentences.
- **Average Pooling:** Simplifies the architecture by compressing information while preserving key semantic details.
- **GloVe Pre-trained Embeddings:** Utilizes pre-trained word vectors to leverage existing knowledge of word relationships and meanings.
- **Neural Network Training:** Employs neural networks with backpropagation for iterative learning and improvement on NLP tasks.

